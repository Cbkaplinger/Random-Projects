{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs a feature importance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "target = 'NRFI'\n",
    "\n",
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "        \n",
    "    #Predict training set:\n",
    "    y_pred = alg.predict(dtrain[predictors])\n",
    "        \n",
    "    #Convert predictions to pandas series\n",
    "    y_pred_series = pd.Series(y_pred)\n",
    "        \n",
    "    #Print model report:\n",
    "    #print(\"\\nModel Report\")\n",
    "    #print(f\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[target].values, y_pred_series))\n",
    "    #print(f\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain[target].values, y_pred_series))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.feature_importances_).sort_values(ascending=False)\n",
    "    fig, ax = plt.subplots()\n",
    "    feat_imp.plot(kind='bar', title='Feature')\n",
    "    #plt.xticks(range(len(feat_imp)), [predictors[i] for i in range(len(feat_imp))])\n",
    "\n",
    "predictors = [x for x in Train5.columns if x not in [target]]\n",
    "xgb1 = XGBRegressor(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "\n",
    "modelfit(xgb1, Train5, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Cross Validation to see the best parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "TrainFeatures = Train5.drop(columns = [\"NRFI\"])\n",
    "TrainLabel = Train5[\"NRFI\"]\n",
    "\n",
    "# Define the parameter grid for RandomForestRegressor\n",
    "param_test1 = {\n",
    "    'n_estimators': range(140, 160, 2),\n",
    "    'min_samples_leaf': range(2, 6, 2),\n",
    "    'max_depth': [10, 20, 2]\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV with RandomForestRegressor\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=140,\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=27\n",
    "    ),\n",
    "    param_grid=param_test1,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch1.fit(TrainFeatures, TrainLabel)\n",
    "\n",
    "print(gsearch1.cv_results_)\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "TrainFeatures = Train5.drop(columns = [\"NRFI\"])\n",
    "TrainLabel = Train5[\"NRFI\"]\n",
    "\n",
    "# Define the parameter grid for GradientBoostingRegressor\n",
    "param_test1 = {\n",
    "    'n_estimators': range(90, 115, 2),\n",
    "    'min_samples_leaf': range(2, 8, 2),\n",
    "    'max_depth': [5, 15, 2]\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV with GradientBoostingRegressor\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(\n",
    "        n_estimators=140,\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=27\n",
    "    ),\n",
    "    param_grid=param_test1,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch1.fit(TrainFeatures, TrainLabel)\n",
    "\n",
    "print(gsearch1.cv_results_)\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "TrainFeatures = Train5.drop(columns = [\"NRFI\"])\n",
    "TrainLabel = Train5[\"NRFI\"]\n",
    "\n",
    "param_test1 = {\n",
    " 'n_estimators': range(40, 70, 2),\n",
    " 'min_child_weight': range(18, 22, 2),\n",
    " 'max_depth': [20, 60, 2],\n",
    " \"scale_pos_weight\": [1, 5, 1]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator=XGBRegressor(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27\n",
    "    ),\n",
    "    param_grid=param_test1,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=4,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "gsearch1.fit(TrainFeatures, TrainLabel)\n",
    "\n",
    "print(gsearch1.cv_results_)\n",
    "print(gsearch1.best_params_)\n",
    "print(gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDKData2024():\n",
    "    eastern_time = datetime.datetime.now(timezone.utc).astimezone(timezone(datetime.timedelta(hours=-5)))\n",
    "    todaysdate = eastern_time.strftime(\"%m-%d-%Y\")\n",
    "    url = 'https://rotogrinders.com/lineups/mlb?site=draftkings'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "    gamelist = []\n",
    "    gamecards = soup.findAll(\"div\", {\"class\": \"game-card-teams\"})\n",
    "    for x in gamecards:\n",
    "        twoteams = x.findAll(\"span\", {\"class\": \"team-nameplate-mascot\"})\n",
    "        roadteam = convert_name(twoteams[0].text)\n",
    "        hometeam = convert_name(twoteams[1].text)\n",
    "        gamekey = \"{}@{}\".format(roadteam,hometeam)\n",
    "        gamelist.append(gamekey)\n",
    "\n",
    "    matchupsdf = pd.DataFrame()\n",
    "    for game in gamelist:\n",
    "        roadteam = game.split(\"@\")[0]\n",
    "        hometeam = game.split(\"@\")[1]\n",
    "        thisdf1 = pd.DataFrame({\"Team\": roadteam, \"Opp\": hometeam, \"RoadTeam\": roadteam, \"HomeTeam\": hometeam},index=[0])\n",
    "        thisdf2 = pd.DataFrame({\"Team\": hometeam, \"Opp\": roadteam, \"RoadTeam\": roadteam, \"HomeTeam\": hometeam},index=[0])\n",
    "        matchupsdf = pd.concat([matchupsdf,thisdf1,thisdf2])\n",
    "        \n",
    "    oppdict = dict(zip(matchupsdf.Team,matchupsdf.Opp))\n",
    "    hometeamdict = dict(zip(matchupsdf.Team,matchupsdf.HomeTeam))\n",
    "    roadteamdict = dict(zip(matchupsdf.Team,matchupsdf.RoadTeam))\n",
    "\n",
    "    disabled_span_list = []\n",
    "    for span in soup.findAll(\"span\", {\"class\": \"player-nameplate disabled\"}):\n",
    "        for a in span.findAll(\"a\"):\n",
    "            disabled_span_list.append(a.text)\n",
    "\n",
    "    spdata = pd.DataFrame()\n",
    "    for div in soup.findAll(\"span\", {\"class\": \"player-nameplate\", \"data-position\": \"SP\"}):\n",
    "        if \"TBD\" in str(div):\n",
    "            playername = \"TBD\"\n",
    "            pos = \"SP\"\n",
    "            sal = 0\n",
    "        else:\n",
    "            for a in div.findAll('a', {'class': 'player-nameplate-name'}):\n",
    "                playername = a.text.strip()\n",
    "\n",
    "            strdiv = str(div)\n",
    "            pos = strdiv[strdiv.find(\"data-position\")+15:strdiv.find(\"data-salary\")-2]\n",
    "            sal = strdiv[strdiv.find(\"data-salary\")+13:strdiv.find(\"<div class = 'player-nameplate-info'>\")-3]\n",
    "        try:\n",
    "            ownership = strdiv[strdiv.find('<span class=\"small muted\" data-auth=\"502\">') + 42:strdiv.find('%')]\n",
    "            ownership = ownership.replace(\"</span>\", \"\")\n",
    "            ownership = ownership.replace(\"</span\", \"\")\n",
    "            ownership = ownership.replace(\"</div>\", \"\")\n",
    "            ownership = ownership.replace(\" \", \"\")\n",
    "        except:\n",
    "            ownership = np.nan\n",
    "\n",
    "        thisspdata = pd.DataFrame([[playername, sal, ownership]], columns = [\"Player\", \"Salary\", \"Ownership\"])\n",
    "        spdata = pd.concat([spdata, thisspdata])\n",
    "\n",
    "    spdata2 = pd.merge(spdata, PID[[\"Name\", \"Team\"]], left_on = [\"Player\"], right_on = [\"Name\"], how = \"left\").rename(columns = {\"Team\": \"PitcherTeam\"})\n",
    "    spdata3 = pd.merge(spdata2, matchupsdf[[\"Team\", \"Opp\"]], left_on = [\"PitcherTeam\"], right_on = [\"Team\"], how = \"left\").drop(columns = [\"Team\"])\n",
    "\n",
    "    opp_spname_dict = dict(zip(spdata3.Opp, spdata3.Player))\n",
    "    opp_spsal_dict = dict(zip(spdata3.Opp, spdata.Salary))\n",
    "    opp_spown_dict = dict(zip(spdata3.Opp, spdata3.Ownership))\n",
    "\n",
    "    ludf = pd.DataFrame()\n",
    "    \n",
    "    for li in soup.findAll(\"li\", {\"class\": \"lineup-card-player\"}):\n",
    "        for a in li.findAll(\"a\", {\"class\": [\"player-nameplate-name\", \"player-nameplate disabled\"]}):\n",
    "            playername = a.text\n",
    "\n",
    "        listring = str(li)\n",
    "        for span in li.find(\"span\", {\"class\": \"small\"}):\n",
    "            luspot = span.text\n",
    "            luspot = luspot.replace(\"\\n\", \"\")\n",
    "            luspot = luspot.strip()\n",
    "            luspot = int(luspot)\n",
    "        pos = listring[listring.find(\"data-position\")+15:listring.find(\"data-salary\")-2]\n",
    "        sal = listring[listring.find(\"data-salary\")+13:listring.find(\"<span class='small'>\")-3]\n",
    "        ownership = ownership.replace(\"</span>\", \"\")\n",
    "        ownership = ownership.replace(\"</span\", \"\")\n",
    "        ownership = ownership.replace(\"</li\", \"\")\n",
    "        ownership = ownership.replace(\"</div>\", \"\")\n",
    "        ownership = ownership.replace(\" \", \"\")\n",
    "\n",
    "        try:\n",
    "            sal = int(sal)\n",
    "        except:\n",
    "            sal = 0\n",
    "        thisludf = pd.DataFrame([[playername, luspot, sal, ownership]], columns = [\"Player\", \"Spot\", \"Sal\", \"Ownership\"])\n",
    "        ludf = pd.concat([ludf, thisludf])\n",
    "\n",
    "    ludf2 = pd.merge(ludf, BID[[\"Name\", \"Team\"]], left_on = [\"Player\"], right_on = [\"Name\"], how = \"left\").rename(columns = {\"Team\": \"BatterTeam\"})\n",
    "    ludf2['BatterTeam'] = ludf2['BatterTeam'].fillna(method='ffill')\n",
    "    ludf2['HomeTeam'] = ludf2['BatterTeam'].map(hometeamdict)\n",
    "    ludf2['RoadTeam'] = ludf2['BatterTeam'].map(roadteamdict)\n",
    "\n",
    "    ludf2_teamlist = list(ludf2[\"BatterTeam\"])\n",
    "\n",
    "    dhteams = []\n",
    "    for x in ludf2_teamlist:\n",
    "        if ludf2_teamlist.count(x) > 11:\n",
    "            if x in dhteams:\n",
    "                pass\n",
    "            else:\n",
    "                dhteams.append(x)\n",
    "\n",
    "    extract_dh = ludf2[ludf2[\"BatterTeam\"].isin(dhteams)]\n",
    "    new_ludf2 = ludf2[~ludf2[\"BatterTeam\"].isin(dhteams)]\n",
    "\n",
    "    new_team_list = []\n",
    "    new_home_list = []\n",
    "    new_road_list = []\n",
    "    runcounter = 0\n",
    "\n",
    "    for x, home, road in zip(extract_dh[\"BatterTeam\"].astype(str), \n",
    "                         extract_dh[\"HomeTeam\"].astype(str), \n",
    "                         extract_dh[\"RoadTeam\"].astype(str)):\n",
    "        if runcounter < 18:\n",
    "            new_team_list.append(x)\n",
    "            new_home_list.append(home)\n",
    "            new_road_list.append(road)\n",
    "            runcounter += 1\n",
    "        else:\n",
    "            new_team_list.append(x + \"2\")\n",
    "            new_home_list.append(home + \"2\")\n",
    "            new_road_list.append(road + \"2\")\n",
    "            runcounter += 1\n",
    "\n",
    "    extract_dh[\"BatterTeam\"] = new_team_list\n",
    "    extract_dh[\"HomeTeam\"] = new_home_list\n",
    "    extract_dh[\"RoadTeam\"] = new_road_list\n",
    "\n",
    "    ludf2 = pd.concat([extract_dh, new_ludf2])\n",
    "    ludf2[\"Opp\"] = ludf2[\"BatterTeam\"].map(oppdict)\n",
    "    ludf2['SP'] = ludf2['BatterTeam'].map(opp_spname_dict)\n",
    "    ludf2['SPSal'] = ludf2['BatterTeam'].map(opp_spsal_dict)\n",
    "    ludf2['SPOwnership'] = ludf2['BatterTeam'].map(opp_spown_dict)\n",
    "    ludf2['Date'] = todaysdate\n",
    "    ludf2['Time'] = np.nan\n",
    "\n",
    "    ludf3 = ludf2[['BatterTeam','RoadTeam','HomeTeam','Time','Spot','Player','Sal','Ownership','Date', \"SP\"]]\n",
    "\n",
    "    dkdata = ludf3.copy()\n",
    "\n",
    "    try:\n",
    "        checknan = dkdata[[\"BatterTeam\", \"SP\"]]\n",
    "        getnans = checknan[[\"SP\"].isna()]\n",
    "        if len(getnans) == 0:\n",
    "            nonans = 1\n",
    "            nanmapdict = {}\n",
    "        else:\n",
    "            nonans = 0\n",
    "            getnans[\"SP\"] = disabled_span_list\n",
    "            nanmapdict = dict(zip(getnans.Team, getnans.SP))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        dkdata[\"SP\"] = np.where(dkdata[\"SP\"].isna(), dkdata[\"BatterTeam\"].map(nanmapdict), dkdata[\"SP\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for i in range(1, len(dkdata) - 1):\n",
    "        if dkdata.loc[i, 'BatterTeam'] != dkdata.loc[i-1, 'BatterTeam']:\n",
    "            if dkdata.loc[i, 'BatterTeam'] != dkdata.loc[i+1, 'BatterTeam']:\n",
    "                dkdata.loc[i, 'BatterTeam'] = np.nan\n",
    "                dkdata.loc[i, 'HomeTeam'] = np.nan\n",
    "                dkdata.loc[i, 'RoadTeam'] = np.nan\n",
    "                dkdata.loc[i, 'SP'] = np.nan\n",
    "\n",
    "    \n",
    "    dkdata[[\"BatterTeam\", \"RoadTeam\", \"HomeTeam\"]] = dkdata[[\"BatterTeam\", \"RoadTeam\", \"HomeTeam\"]].fillna(method='ffill')\n",
    "    dkdata = dkdata.drop_duplicates(subset = [\"BatterTeam\", \"SP\"], keep = \"first\")\n",
    "    dkdata = dkdata.drop(columns = [\"Time\", \"Sal\", \"Ownership\"])\n",
    "\n",
    "    dkdata['BatterTeam'] = dkdata['BatterTeam'].replace('ARI', 'AZ')\n",
    "    dkdata['RoadTeam'] = dkdata['RoadTeam'].replace('ARI', 'AZ')\n",
    "    dkdata['HomeTeam'] = dkdata['HomeTeam'].replace('ARI', 'AZ')\n",
    "\n",
    "    dkdata['Date'] = pd.to_datetime(dkdata['Date'])\n",
    "    dkdata['Date'] = dkdata['Date'].dt.strftime('%Y-%m-%d')\n",
    "    dkdata = dkdata.set_index(\"Date\")\n",
    "    dkdata = dkdata[[\"BatterTeam\", \"RoadTeam\", \"HomeTeam\", \"SP\"]]\n",
    "\n",
    "    return(dkdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
